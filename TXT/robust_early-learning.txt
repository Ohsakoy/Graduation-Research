深奥学習の成功には、高品質なannotatinoが施された大量のデータによりものが大きい
これは、実現不可能
大規模なデータセットにはノイズが多いラベル
deep netoworksは大きな学習容量,強い記憶力を持つため,オーバーフィットする
ドロップアウト、重み減衰の正則化では、対応できない
そこで、早期停止法を用いる
どのパラメータがきれいなラベルを持つデータに適合するために重要なのか、また、どのパラメータがノイズの多いラベルを持つデータに適合する傾向があるのかを見つける
パラメータの種類に応じて異なる更新ルールを設計することで，ノイズの多いラベルによる副作用を軽減

related work
早期停止は、シンプルだが効果的
オーバーフィッティングを避けるために、収束する前に学習を停止
ロバスト性と一般化を向上させるために、既存の手法で広く用いられています
本論文では、ノイズの多いラベルを用いた学習に焦点を当て、ノイズの多いラベルの副作用を軽減するための重要,非重要パラメータを見つけ、汎化性能を大幅に向上させることを目指している。
多くの研究が、ノイズのあるラベルを用いた学習のための様々な方法を提案している
ノイズ遷移行列を利用,surrogate loss functions,meta-learning, employing the small loss trick 

2.1
ニューラルネットワークの最適化手法
クリーンラベルを知識背景とする典型的な教師付き学習の最適化問題を分析
loss function L
W:パラメータ
m:パラメータの総数
λ:正則化パラメータ
η:学習率
S*:Sからランダムにサンプリングされた部分集合

3.1 最適化の判断基準 
∇L(W; S)=0の時にWで最適性が達成される
スカラーG0 (1)を利用して、最適性を確認することができます。なお、この新しい最適化基準は十分ではあるが、必須ではない

3.2パラメータの判断 
最適性を得るためには、G'(1)の値をゼロにする必要があります。
w:パラメータ
∇L:勾配
g_i:判定基準
gi の値が大きい場合、wi は重要なパラメータとみなされる。
逆に、giの値が小さい場合、wiは非クリティカルなパラメータとみなされる。
これを更新すると、ノイズの多いラベルをフィットさせる傾向があります。

L(wi ; S)の勾配を臨界性の基準として直接使用することの根本的な問題を特定することができる。
勾配情報のみを利用する場合、パラメータwiの値を無視する
しかし、その値がゼロまたはゼロに近い場合、そのパラメータは不活性化されます。また、最適化のためには非重要です
early stoppingを使用している
深層ネットワークは、初期の学習では主にきれいなラベルに適合する
ノイズの多いラベルが存在していても、パラメータの臨界を分析するために基準を使用することができる
g_i=0のとき注意すべき
(1)∇L(wi ; S) = 0
(2)wi = 0
(3)L(wi ; S) = 0とwi = 0

gi＝0の場合は、∇L（wi ; S）の値が0ではないこと,新たな最適化基準が必要ない
提案手法の有効性に影響を与えていない。

3.3異なる更新ルール
ラベルのノイズ率を利用して、パラメータを重要なものとそうでないものに分ける方法
クリティカル・パラメータの数は、ノイズ・レートと負の相関がある
クリティカルパラメータを特定するために、ノイズ率を利用
クリティカルパラメータの数
mc = (1 - τ )m.
ノイズ率をτ
重要なパラメータと非重要なパラメータは，それぞれWcとWn

クリティカルなものWcはロバストな正の更新
式(5)からわかるように、勾配減衰係数を1 - τに設定することで、学習過程での過信的な下降ステップを防ぐことができる

クリティカルではないパラメータWnについては、重みの減衰のみを利用して更新します
Robust positive updateは、勾配を利用して重要なものを更新することで、ディープネットワークがきれいなラベルを記憶するのを助ける

2つの更新ルールを使用することで、目的、すなわちノイズの多いラベルの副作用を減らし、クリーンなラベルの記憶を強化するという目的を達成できる

4実証実験
MNIST,F-MNIST,CIFAR-10,CIFAR-100のデータセットで実験を行う
全てのデータセットにおいて、先行研究に従い、検証セットとして10％の学習データを除外する
これは早期停止のため
four types of synthetic label noise, symmetric noise, asymmetric noise, pairflip noise and instance-dependent noise を考える
対称型ノイズ：各クラスのラベルを他のクラスの誤ったラベルと一様に反転させることで発生するラベルノイズ
非対称ノイズ：似たようなクラスの集合の中でラベルを反転させることで発生するノイズ
ペアフリップノイズ：各クラスを隣のクラスに反転させるノイズ
インスタンスノイズ：インスタンスが誤表示される確率がその特徴に依存するという、かなり現実的なノイズ

MNISTでは，LeNet (LeCun et al., 1998)をバッチサイズ32
F-MNISTでは，ResNet-50 (He et al., 2015)をバッチサイズ32
CIFAR-10とCIFAR-100では，バッチサイズ64のResNet-50を学習

すべての学習において、optimizer:SGD,momentum:0.9、weight decay:10^-3に設定
初期学習率は 10^-2 に設定
epoch:100
PyTorch 1.2.0とCUDA 10.0で実装

4.2　比較手法
(1) CE，ノイズの多いデータセットに対してクロスエントロピー損失を用いてディープニューラルネットワークを学習する手法
(2) GCE，平均絶対誤差損失とクロスエントロピー損失を統合して，ノイズの多いラベルを処理する．
(3) 情報理論の観点からノイズの多いラベルに対処する DMI
(4) APL, 2つの相互強化型ロバスト損失関数を組み合わせたもの
(5) MentorNet, ノイズの多いデータをフィルタリングするカリキュラムを学習
(6) Co-teaching , 2つのネットワークを維持し、小さな損失でインスタンスのクロストレーニングを行う
(7) Co-teaching+ , 2つのネットワークを維持し、予測の不一致データの中から損失の小さいインスタンスを見つけて学習
(8) S2E , 自動機械学習を利用してノイズの多いラベルを処理
(9) Forward, これはノイズ遷移行列を推定して学習損失を補正するもの
(10) T-Revision , 重要度再重み付け技術を採用し、スラック変数を導入してノイズ遷移行列を修正
(11) Joint, ネットワークパラメータとサンプルラベルを共同で最適化

4.3 ABLATION STUDY
パラメータの種類数や勾配減衰係数を決めるためにはノイズ率が必要
インスタンスノイズのノイズ率を推定することは難しい
我々の提案する手法は、ノイズ率の推定結果に影響されないことを提示する。
提案手法は，ノイズ率の推定結果に対してロバストである
この論文では，非クリティカルなパラメータの割合をノイズレートに設定している
多くの場合，配置された定数とラベルノイズ率は数値的には等しい．
しかし、ノイズの多い検証セットでは、探索範囲が膨大になるため、適切な定数を見つけるのは複雑です。
それに反して、ノイズ率は常に効果的に推定することができます。
したがって、非重要なパラメータの割合は、ノイズ率と同じであると仮定することは合理的であり、実現可能である。

4.4 ノイズの多いデータセットでの分類性能


本論文では、宝くじ仮説に基づいて、クリティカルなパラメータと非クリティカルなパラメータを区別して、きれいなラベルを付けるための新しい方法を提供する
パラメータの種類に応じて異なる更新規則を提案し，早期停止による副作用を軽減する．
提案手法は、ノイズの多いラベルを用いた学習に非常に有効である
このことは、様々なタイプのラベルノイズを含む合成データセットおよび実世界のデータセットを用いた実験によって裏付けられる。
我々の手法はシンプルであり、他の手法と直交している。
これは、ノイズの多いラベルを用いた学習というテーマに新たな可能性をもたらす